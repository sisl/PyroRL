{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyroRL A new reinforcement learning OpenAI Gym environment built for the simulation of wildfire evacuation. Motivation One of the main effects of climate change in the world today is the increased frequency and intensity of wildfires . This reality has led to an increase in interest in wildfire response as demonstrated by recent work done by the Stanford Intelligent Systems Lab . This Python package builds off of existing research within wildfire disaster response. The central motivation is to simulate an environment over a wide area with residential sectors and a spreading wildfire. Unlike previous research , which focused on monitoring the spread of fire, this environment focuses on developing a community\u2019s response to the wildfire in the form of evacuation in an environment that presumes perfect information about the fire\u2019s current state. More specifically, the motivation will be to identify areas that must be evacuated and the ideal path to \u201csafe areas.\u201d We hope to enable researchers to use deep reinforcement learning techniques to accomplish this task.","title":"Home"},{"location":"#pyrorl","text":"A new reinforcement learning OpenAI Gym environment built for the simulation of wildfire evacuation.","title":"PyroRL"},{"location":"#motivation","text":"One of the main effects of climate change in the world today is the increased frequency and intensity of wildfires . This reality has led to an increase in interest in wildfire response as demonstrated by recent work done by the Stanford Intelligent Systems Lab . This Python package builds off of existing research within wildfire disaster response. The central motivation is to simulate an environment over a wide area with residential sectors and a spreading wildfire. Unlike previous research , which focused on monitoring the spread of fire, this environment focuses on developing a community\u2019s response to the wildfire in the form of evacuation in an environment that presumes perfect information about the fire\u2019s current state. More specifically, the motivation will be to identify areas that must be evacuated and the ideal path to \u201csafe areas.\u201d We hope to enable researchers to use deep reinforcement learning techniques to accomplish this task.","title":"Motivation"},{"location":"contribution-guide/","text":"Contribution Guide To get started on contributing, check out the below guide on how to get the package installed locally, how to update this documentation website, and more. How to Set-Up To set up our codebase, create a virtual environment and install a local copy of the package. python3 -m venv env source env/bin/activate cd wildfire_evac/ pip install . Package Deployment All of the details of the package can be found in wildfire_evac/setup.py . This file defines attributes such as the name of the package, the version number, and the dependencies needed to use the package. Next, you build the distribution archives. Archives are compressed files that allow the package to be deployed across multiple platforms. You run the following command to generate the distribution files: python3 setup.py sdist Using the above command should generate a dist folder, which contains the compressed distribution files. Finally, we can publish it to the official PyPi repository using a package called twine : twine upload dist/* Doing the * will upload all of the compressed distribution files, so ideally, we would clear out all of them and then have only one set of distribution files. Testing We use pytest for our backend tests. To keep the state of our package as small as possible, we don't include pytest . Thus, make sure to install the package before running. pip install pytest python3 -m pytest -s Continuous Integration We use GitHub Actions to automatically run our entire test suite upon each push . Check out the file .github/workflows/testing.yml to edit this job. Documentation We use MkDocs for our documentation. To set up and make edits to our documentation, first install the MkDocs package: pip install mkdocs Then, making sure you're in the same directory as the mkdocs.yml configuration file, you can start the server by running the command: mkdocs serve For more tips, check out their documentation .","title":"Contribution Guide"},{"location":"contribution-guide/#contribution-guide","text":"To get started on contributing, check out the below guide on how to get the package installed locally, how to update this documentation website, and more.","title":"Contribution Guide"},{"location":"contribution-guide/#how-to-set-up","text":"To set up our codebase, create a virtual environment and install a local copy of the package. python3 -m venv env source env/bin/activate cd wildfire_evac/ pip install .","title":"How to Set-Up"},{"location":"contribution-guide/#package-deployment","text":"All of the details of the package can be found in wildfire_evac/setup.py . This file defines attributes such as the name of the package, the version number, and the dependencies needed to use the package. Next, you build the distribution archives. Archives are compressed files that allow the package to be deployed across multiple platforms. You run the following command to generate the distribution files: python3 setup.py sdist Using the above command should generate a dist folder, which contains the compressed distribution files. Finally, we can publish it to the official PyPi repository using a package called twine : twine upload dist/* Doing the * will upload all of the compressed distribution files, so ideally, we would clear out all of them and then have only one set of distribution files.","title":"Package Deployment"},{"location":"contribution-guide/#testing","text":"We use pytest for our backend tests. To keep the state of our package as small as possible, we don't include pytest . Thus, make sure to install the package before running. pip install pytest python3 -m pytest -s","title":"Testing"},{"location":"contribution-guide/#continuous-integration","text":"We use GitHub Actions to automatically run our entire test suite upon each push . Check out the file .github/workflows/testing.yml to edit this job.","title":"Continuous Integration"},{"location":"contribution-guide/#documentation","text":"We use MkDocs for our documentation. To set up and make edits to our documentation, first install the MkDocs package: pip install mkdocs Then, making sure you're in the same directory as the mkdocs.yml configuration file, you can start the server by running the command: mkdocs serve For more tips, check out their documentation .","title":"Documentation"},{"location":"how-it-works/","text":"How it works Overview of State Space We represent the state space as an (5 by num_rows by num_cols ) np.array . Specifically, each of the 5 np.array s identifies a specific attribute: 0 = FIRE_INDEX \u2013 whether or not a square in the grid world is on fire or not 1 = FUEL_INDEX \u2013 the amount of fuel in the square, which is used to determine if an area will be enflamed or not 2 = POPULATED_INDEX \u2013 whether or not a square is a populated area or not 3 = EVACUATING_INDEX \u2013 whether or not a square is evacuating or not 4 = PATHS_INDEX \u2013 the number of paths a square is a part of Overview of Action Space Our simulation assumes that our action taker oversees the entire grid world, as this models how evacuation would occur in real life. The action space is defined as the number of possible paths to evacuate by, plus an extra action to do nothing. When an agent takes an action, three steps occur: The wildfire model propagates and expands to neighboring cells The available paths are updated, as well as which areas are evacuating or not The environment calculates the total amount of reward accumulated Spread of Wildfire Each fire cell has an initial fuel level drawn from a normal distribution. The spread of the wildfire is stochastic.","title":"How It Works"},{"location":"how-it-works/#how-it-works","text":"","title":"How it works"},{"location":"how-it-works/#overview-of-state-space","text":"We represent the state space as an (5 by num_rows by num_cols ) np.array . Specifically, each of the 5 np.array s identifies a specific attribute: 0 = FIRE_INDEX \u2013 whether or not a square in the grid world is on fire or not 1 = FUEL_INDEX \u2013 the amount of fuel in the square, which is used to determine if an area will be enflamed or not 2 = POPULATED_INDEX \u2013 whether or not a square is a populated area or not 3 = EVACUATING_INDEX \u2013 whether or not a square is evacuating or not 4 = PATHS_INDEX \u2013 the number of paths a square is a part of","title":"Overview of State Space"},{"location":"how-it-works/#overview-of-action-space","text":"Our simulation assumes that our action taker oversees the entire grid world, as this models how evacuation would occur in real life. The action space is defined as the number of possible paths to evacuate by, plus an extra action to do nothing. When an agent takes an action, three steps occur: The wildfire model propagates and expands to neighboring cells The available paths are updated, as well as which areas are evacuating or not The environment calculates the total amount of reward accumulated","title":"Overview of Action Space"},{"location":"how-it-works/#spread-of-wildfire","text":"Each fire cell has an initial fuel level drawn from a normal distribution. The spread of the wildfire is stochastic.","title":"Spread of Wildfire"},{"location":"quickstart/","text":"Quickstart Basic Usage First, download our Python package: pip install wildfire-evac To use our environment, you need to define five parameters: num_rows and num_cols \u2013 these two integers define a ( num_rows , num_cols ) grid world populated_areas \u2013 an array of [x, y] coordinates that indicate the location of the populated areas paths \u2013 an array of paths. Each path is an array of [x, y] coordinates that indicate each square in a path paths_to_pops \u2013 a dictionary that maps paths to which populated areas can use those paths. The keys are integers that represent the index of the path in the paths array. The values are arrays of integers that represent the populated area in the populated_areas array. You can see an example below: num_rows, num_cols = 10, 10 populated_areas = np.array([[1,2],[4,8], [6,4], [8, 7]]) paths = np.array([[[1,0],[1,1]], [[2,2],[3,2],[4,2],[4,1],[4,0]], [[2,9],[2,8],[3,8]], [[5,8],[6,8],[6,9]], [[7,7], [6,7], [6,8], [6,9]], [[8,6], [8,5], [9,5]], [[8,5], [9,5], [7,5],[7,4]]], dtype=object) paths_to_pops = {0:[[1,2]], 1:[[1,2]], 2: [[4,8]], 3:[[4,8]], 4:[[8, 7]], 5:[[8, 7]], 6:[[6,4]]} Using these parameters, you can then define them as kwargs and use gymnasium.make to create the environment: kwargs = { 'num_rows': num_rows, 'num_cols': num_cols, 'populated_areas': populated_areas, 'paths': paths, 'paths_to_pops': paths_to_pops } env = gymnasium.make('wildfire_evac/WildfireEvacuation-v0', **kwargs) The wildfire environment has the following action and state/observation space: action_space: [0, number of paths] observation_space: (5, num_rows, num_cols) tensor, where every value in each dimension is between [0, 200] Now, with all of this information, we can create a loop that samples an action from the action space, takes a step with the action, and renders the environment: env.reset() for _ in range(10): # Take action and observation action = env.action_space.sample() observation, reward, terminated, truncated, info = env.step(action) # Render environment and print reward env.render() print(\"Reward: \" + str(reward)) # Generate the gif env.generate_gif() Finally, we can see that the function generate_gif allows us to collate all of the visualizations generated by the render function and stitch them together into a GIF: Your browser does not support the video tag. Use with Stable Baselines 3 Stable Baselines 3 is a popular library for reinforcement learning algorithms. You can use Stable Baseline algorithms to find optimal policies for the wildfire evacuation problem. Given that our environment conforms to the gymnasium specification, training a model is no different than with other environments: # Train a model and delete model = DQN(\"MlpPolicy\", env, verbose=1) model.learn(total_timesteps=1000, log_interval=4) model.save(\"sample_baseline\") del model The same is true for using the model during evaluation: # Load and reset the environment model = DQN.load(\"sample_baseline\") obs, info = env.reset() # Run a simple loop of the environment for _ in range(10): action, _states = model.predict(obs, deterministic=True) observation, reward, terminated, truncated, info = env.step(int(action)) # Render environment and print reward env.render() print(\"Reward: \" + str(reward))","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#basic-usage","text":"First, download our Python package: pip install wildfire-evac To use our environment, you need to define five parameters: num_rows and num_cols \u2013 these two integers define a ( num_rows , num_cols ) grid world populated_areas \u2013 an array of [x, y] coordinates that indicate the location of the populated areas paths \u2013 an array of paths. Each path is an array of [x, y] coordinates that indicate each square in a path paths_to_pops \u2013 a dictionary that maps paths to which populated areas can use those paths. The keys are integers that represent the index of the path in the paths array. The values are arrays of integers that represent the populated area in the populated_areas array. You can see an example below: num_rows, num_cols = 10, 10 populated_areas = np.array([[1,2],[4,8], [6,4], [8, 7]]) paths = np.array([[[1,0],[1,1]], [[2,2],[3,2],[4,2],[4,1],[4,0]], [[2,9],[2,8],[3,8]], [[5,8],[6,8],[6,9]], [[7,7], [6,7], [6,8], [6,9]], [[8,6], [8,5], [9,5]], [[8,5], [9,5], [7,5],[7,4]]], dtype=object) paths_to_pops = {0:[[1,2]], 1:[[1,2]], 2: [[4,8]], 3:[[4,8]], 4:[[8, 7]], 5:[[8, 7]], 6:[[6,4]]} Using these parameters, you can then define them as kwargs and use gymnasium.make to create the environment: kwargs = { 'num_rows': num_rows, 'num_cols': num_cols, 'populated_areas': populated_areas, 'paths': paths, 'paths_to_pops': paths_to_pops } env = gymnasium.make('wildfire_evac/WildfireEvacuation-v0', **kwargs) The wildfire environment has the following action and state/observation space: action_space: [0, number of paths] observation_space: (5, num_rows, num_cols) tensor, where every value in each dimension is between [0, 200] Now, with all of this information, we can create a loop that samples an action from the action space, takes a step with the action, and renders the environment: env.reset() for _ in range(10): # Take action and observation action = env.action_space.sample() observation, reward, terminated, truncated, info = env.step(action) # Render environment and print reward env.render() print(\"Reward: \" + str(reward)) # Generate the gif env.generate_gif() Finally, we can see that the function generate_gif allows us to collate all of the visualizations generated by the render function and stitch them together into a GIF: Your browser does not support the video tag.","title":"Basic Usage"},{"location":"quickstart/#use-with-stable-baselines-3","text":"Stable Baselines 3 is a popular library for reinforcement learning algorithms. You can use Stable Baseline algorithms to find optimal policies for the wildfire evacuation problem. Given that our environment conforms to the gymnasium specification, training a model is no different than with other environments: # Train a model and delete model = DQN(\"MlpPolicy\", env, verbose=1) model.learn(total_timesteps=1000, log_interval=4) model.save(\"sample_baseline\") del model The same is true for using the model during evaluation: # Load and reset the environment model = DQN.load(\"sample_baseline\") obs, info = env.reset() # Run a simple loop of the environment for _ in range(10): action, _states = model.predict(obs, deterministic=True) observation, reward, terminated, truncated, info = env.step(int(action)) # Render environment and print reward env.render() print(\"Reward: \" + str(reward))","title":"Use with Stable Baselines 3"},{"location":"reference/","text":"Reference You can find our codebase at the following Github link . If you have any questions about how to use our package or contribute to the project, create an issue or reach out directly to our team: clpondoc@stanford.edu jobrien3@stanford.edu joeytg@stanford.edu","title":"Reference"},{"location":"reference/#reference","text":"You can find our codebase at the following Github link . If you have any questions about how to use our package or contribute to the project, create an issue or reach out directly to our team: clpondoc@stanford.edu jobrien3@stanford.edu joeytg@stanford.edu","title":"Reference"}]}